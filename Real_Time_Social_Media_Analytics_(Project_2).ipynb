{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy gradio wordcloud matplotlib textblob pandas pytrends plotly seaborn vaderSentiment requests feedparser -q\n",
        "!pip install -U kaleido\n",
        "!pip install praw\n",
        "\n",
        "import praw\n",
        "import tweepy\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import datetime\n",
        "from pytrends.request import TrendReq\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import requests\n",
        "import feedparser\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# ===== Twitter API Credentials =====\n",
        "consumer_key = 'sWoK8Zca1Pn38MSpa1CZUShgU'\n",
        "consumer_secret = 'FTQCrruIkiw10Ggr78mMmGSuIpp8ka5aL86FxIg0gy2SqWnGdr'\n",
        "access_token = '1923239602196017152-o0S1qy4oEvy0Y3bG3KWzwMY25FI1Ly'\n",
        "access_token_secret = 'amiQiXNKboVwYmszwpdvbZjI3XMGcpBFW1WRMKI3FWHNN'\n",
        "bearer_token = 'AAAAAAAAAAAAAAAAAAAAABgE1wEAAAAAVSU3BI0pjab5IVx5bkSaFLOKdVA%3DVu7nsZrN9yRVdOPBTSfxADNb3VfhADCcze2ZzO5nSVFE7dVca5'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "pytrends = TrendReq(hl='en-US', tz=330)\n",
        "\n",
        "# Reddit API Setup\n",
        "reddit = praw.Reddit(\n",
        "    client_id='sykN1Y1UzM9woFTmpVqk1g',\n",
        "    client_secret='1zQjNZFSLSnnDxY78ia-zIqTwhlfrQ',\n",
        "    user_agent='MyDataFetcher script by /u/your_reddit_username'\n",
        ")\n",
        "\n",
        "### Twitter Functions ###\n",
        "\n",
        "def fetch_tweets(query, max_results=50):\n",
        "    tweets = []\n",
        "    try:\n",
        "        response = client.search_recent_tweets(\n",
        "            query=query,\n",
        "            max_results=max_results,\n",
        "            tweet_fields=[\"text\", \"lang\"]\n",
        "        )\n",
        "        if response.data:\n",
        "            for tweet in response.data:\n",
        "                if tweet.lang == \"en\":\n",
        "                    tweets.append(tweet.text)\n",
        "    except Exception as e:\n",
        "        tweets.append(f\"Error fetching tweets: {e}\")\n",
        "    return tweets\n",
        "\n",
        "def analyze_sentiment(tweets):\n",
        "    sentiment_data = {'positive': 0, 'neutral': 0, 'negative': 0}\n",
        "    sentiments = []\n",
        "    for tweet in tweets:\n",
        "        analysis = TextBlob(tweet)\n",
        "        polarity = analysis.sentiment.polarity\n",
        "        if polarity > 0:\n",
        "            sentiment = 'positive'\n",
        "            sentiment_data['positive'] += 1\n",
        "        elif polarity == 0:\n",
        "            sentiment = 'neutral'\n",
        "            sentiment_data['neutral'] += 1\n",
        "        else:\n",
        "            sentiment = 'negative'\n",
        "            sentiment_data['negative'] += 1\n",
        "        sentiments.append((tweet, sentiment))\n",
        "    return sentiment_data, sentiments\n",
        "\n",
        "def plot_sentiment(sentiment_data):\n",
        "    labels = sentiment_data.keys()\n",
        "    sizes = sentiment_data.values()\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=['green', 'grey', 'red'])\n",
        "    plt.axis('equal')\n",
        "    plt.title('Sentiment Distribution')\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "    plt.savefig(temp_file.name)\n",
        "    plt.close()\n",
        "    return temp_file.name\n",
        "\n",
        "def generate_wordcloud(tweets):\n",
        "    text = ' '.join(tweets)\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud')\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "    plt.savefig(temp_file.name)\n",
        "    plt.close()\n",
        "    return temp_file.name\n",
        "\n",
        "def export_to_csv(tweet_data):\n",
        "    df = pd.DataFrame(tweet_data, columns=[\"Tweet\", \"Sentiment\"])\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode='w', newline='', encoding='utf-8')\n",
        "    df.to_csv(temp_file.name, index=False)\n",
        "    return temp_file.name\n",
        "\n",
        "def analyze_twitter(query):\n",
        "    tweets = fetch_tweets(query)\n",
        "    if not tweets:\n",
        "        return \"No tweets found or API error.\", None, None, None, None\n",
        "    sentiment_data, tweet_data = analyze_sentiment(tweets)\n",
        "    sentiment_chart = plot_sentiment(sentiment_data)\n",
        "    wordcloud_chart = generate_wordcloud([t[0] for t in tweet_data])\n",
        "    csv_path = export_to_csv(tweet_data)\n",
        "    tweet_samples = '\\n\\n'.join([f\"ğŸ—¨ï¸ {t[0]}\\nSentiment: {t[1]}\" for t in tweet_data[:10]])\n",
        "    return f\"Fetched {len(tweets)} tweets.\", sentiment_chart, wordcloud_chart, tweet_samples, csv_path\n",
        "\n",
        "### Google Trends + News + Reddit Functions ###\n",
        "\n",
        "def fetch_trends(keywords):\n",
        "    try:\n",
        "        pytrends.build_payload(keywords, cat=0, timeframe='now 1-H', geo='', gprop='')\n",
        "        df = pytrends.interest_over_time()\n",
        "        if not df.empty:\n",
        "            df = df.drop(columns='isPartial')\n",
        "            df.reset_index(inplace=True)\n",
        "            return df\n",
        "        else:\n",
        "            return pd.DataFrame()\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def plot_trends(df, keywords):\n",
        "    fig = px.line(df, x='date', y=keywords, title=\"Real-Time Google Trends\")\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "    fig.write_image(temp_file.name)\n",
        "    return temp_file.name\n",
        "\n",
        "def fetch_news_headlines(keyword):\n",
        "    url = f\"https://news.google.com/rss/search?q={keyword}&hl=en-US&gl=US&ceid=US:en\"\n",
        "    feed = feedparser.parse(url)\n",
        "    headlines = [entry.title for entry in feed.entries[:10]]  # top 10 headlines\n",
        "    return headlines\n",
        "\n",
        "def analyze_news_sentiment(headlines):\n",
        "    sentiment_results = []\n",
        "    for headline in headlines:\n",
        "        score = vader_analyzer.polarity_scores(headline)\n",
        "        sentiment_results.append({\n",
        "            'headline': headline,\n",
        "            'neg': score['neg'],\n",
        "            'neu': score['neu'],\n",
        "            'pos': score['pos'],\n",
        "            'compound': score['compound']\n",
        "        })\n",
        "    return pd.DataFrame(sentiment_results)\n",
        "\n",
        "def plot_news_sentiment_distribution(df_sentiment, keyword):\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.histplot(df_sentiment['compound'], bins=10, kde=True)\n",
        "    plt.title(f\"Sentiment Score Distribution for News on '{keyword}'\")\n",
        "    plt.xlabel('Compound Sentiment Score')\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "    plt.savefig(temp_file.name)\n",
        "    plt.close()\n",
        "    return temp_file.name\n",
        "\n",
        "def fetch_reddit_posts(keyword, limit=10):\n",
        "    posts = []\n",
        "    for submission in reddit.subreddit('all').search(keyword, limit=limit):\n",
        "        posts.append({\n",
        "            'title': submission.title,\n",
        "            'score': submission.score,\n",
        "            'url': submission.url,\n",
        "            'created_utc': datetime.datetime.utcfromtimestamp(submission.created_utc),\n",
        "            'num_comments': submission.num_comments,\n",
        "            'subreddit': submission.subreddit.display_name\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def analyze_trends_news_reddit(keywords_str):\n",
        "    import traceback\n",
        "    keywords = [kw.strip() for kw in keywords_str.split(\",\") if kw.strip()]\n",
        "    if not keywords:\n",
        "        return \"Please enter at least one keyword.\", None, None, None, None\n",
        "\n",
        "    try:\n",
        "        # Google Trends\n",
        "        trend_data = fetch_trends(keywords)\n",
        "        if trend_data.empty:\n",
        "            trend_img = None\n",
        "            trend_status = \"No Google Trends data found.\"\n",
        "        else:\n",
        "            trend_img = plot_trends(trend_data, keywords)\n",
        "            trend_status = \"Google Trends data fetched.\"\n",
        "    except Exception as e:\n",
        "        trend_img = None\n",
        "        trend_status = f\"Google Trends error: {e}\\n{traceback.format_exc()}\"\n",
        "\n",
        "    first_kw = keywords[0]\n",
        "\n",
        "    try:\n",
        "        # News headlines + sentiment\n",
        "        headlines = fetch_news_headlines(first_kw)\n",
        "        if not headlines:\n",
        "            news_status = \"No news headlines found.\"\n",
        "            news_sentiment_img = None\n",
        "            news_df = pd.DataFrame()\n",
        "        else:\n",
        "            news_df = analyze_news_sentiment(headlines)\n",
        "            news_sentiment_img = plot_news_sentiment_distribution(news_df, first_kw)\n",
        "            news_status = f\"Fetched {len(headlines)} news headlines for '{first_kw}'.\"\n",
        "    except Exception as e:\n",
        "        news_status = f\"News fetching error: {e}\\n{traceback.format_exc()}\"\n",
        "        news_sentiment_img = None\n",
        "        news_df = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        # Reddit posts\n",
        "        reddit_posts = fetch_reddit_posts(first_kw, limit=10)\n",
        "        if not reddit_posts:\n",
        "            reddit_status = \"No Reddit posts found.\"\n",
        "            reddit_str = \"\"\n",
        "        else:\n",
        "            reddit_status = f\"Fetched {len(reddit_posts)} Reddit posts.\"\n",
        "            reddit_str = '\\n\\n'.join(\n",
        "                [f\"Title: {p['title']}\\nSubreddit: {p['subreddit']}\\nScore: {p['score']}\\nComments: {p['num_comments']}\\nDate: {p['created_utc'].strftime('%Y-%m-%d %H:%M:%S')}\\nURL: {p['url']}\"\n",
        "                for p in reddit_posts]\n",
        "            )\n",
        "    except Exception as e:\n",
        "        reddit_status = f\"Reddit fetching error: {e}\\n{traceback.format_exc()}\"\n",
        "        reddit_str = \"\"\n",
        "\n",
        "    summary = f\"Trends: {trend_status}\\nNews: {news_status}\\nReddit: {reddit_status}\"\n",
        "    return summary, trend_img, news_sentiment_img, reddit_str\n",
        "\n",
        "### Gradio UI ###\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Real Time Social Media Analysis Dashboard\")\n",
        "    with gr.Tab(\"Twitter Sentiment Analysis\"):\n",
        "        with gr.Row():\n",
        "            twitter_input = gr.Textbox(label=\"Enter Keyword or Hashtag (without #)\", placeholder=\"e.g. Bitcoin\", lines=1)\n",
        "            twitter_btn = gr.Button(\"Analyze Twitter\")\n",
        "        twitter_status = gr.Textbox(label=\"Status / Summary\", interactive=False)\n",
        "        twitter_sentiment_img = gr.Image(label=\"Sentiment Distribution\")\n",
        "        twitter_wordcloud_img = gr.Image(label=\"Word Cloud\")\n",
        "        twitter_sample_tweets = gr.Textbox(label=\"Sample Tweets with Sentiment\", lines=10, interactive=False)\n",
        "        twitter_csv = gr.File(label=\"Download CSV of Tweets & Sentiment\")\n",
        "\n",
        "        twitter_btn.click(fn=analyze_twitter, inputs=twitter_input,\n",
        "                          outputs=[twitter_status, twitter_sentiment_img, twitter_wordcloud_img, twitter_sample_tweets, twitter_csv])\n",
        "\n",
        "    with gr.Tab(\"Google Trends + News + Reddit\"):\n",
        "        with gr.Row():\n",
        "            trends_input = gr.Textbox(label=\"Enter Keywords (comma separated)\", placeholder=\"bitcoin, ethereum, crypto\", lines=1)\n",
        "            trends_btn = gr.Button(\"Analyze Trends & News & Reddit\")\n",
        "        trends_status = gr.Textbox(label=\"Status Summary\", interactive=False)\n",
        "        trends_img = gr.Image(label=\"Google Trends Chart\")\n",
        "        news_sentiment_img = gr.Image(label=\"News Sentiment Distribution\")\n",
        "        reddit_posts_text = gr.Textbox(label=\"Reddit Posts\", lines=15, interactive=False)\n",
        "\n",
        "        trends_btn.click(fn=analyze_trends_news_reddit, inputs=trends_input,\n",
        "                         outputs=[trends_status, trends_img, news_sentiment_img, reddit_posts_text])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o-fB3fM5InKu",
        "outputId": "77969689-2c4e-48bd-c6ab-0cba6aa619f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://26bb868601312d6321.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://26bb868601312d6321.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}